{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECBM4040 Assignment 1, Task 4: Questions\n",
    "\n",
    "1) What is the difference between the SVM method and a neural network, assuming that both work with the same number of training samples N?\n",
    "\n",
    "   Your answer: ***SVM tries to moves towards a global optimum in most cases and not local minima, and the math behind SVMs are more well-defined. Neural nets are parametric while SVMs are non-parametic. For the same training samples, the difference in accuracy depends on the types of SVM and Neural nets trained. A basic SVM without kernel can be considered equivalent of a single layer perceptron.***\n",
    "   \n",
    "\n",
    "\n",
    "2) Why is the ReLU activation function used the most often in neural networks for computer vision?\n",
    "\n",
    "   Your answer: ***ReLu is most commonly used as an activation function as it does not face the vanishing gradient problem and helps faster convergence. ReLu also produces sparsity in output and is computationally less expensive (very less mathematical operations) making the model more efficient.***\n",
    "   \n",
    "\n",
    "3) Describe your best model in the implementation of the two-layer neural network. Describe your starting point, how you tuned  the hyperparameters, which stategies you did you use to improve the network, show the results of intermediate and final steps.\n",
    "\n",
    "   Your answer: Best model implementation:\n",
    "   \n",
    "   \n",
    "   ***Starting point: Model uses an MLP function with a Stochastic gradient descent with velocity and momentum. I started with an accuracy of about 10%.***\n",
    "\n",
    "   ***Dimension reduction methods: In order to improve accuracy, I tried dimension reduction using PCA( same as part 3) and a small improvement in accuracy of about 2% was obtained. ***\n",
    "   \n",
    "  *** Hyperparameter tuning: This was the most effective in improvement of accuracy. By decreasing the weight scale, the accuracy improved by over 10% and this was probably because tuning the inital weight values aids in a better start for training the model. Next, I changed the hidden_dims in the model and then went on to change the bacth size. Adjusting the batch size by increase/decrease and arriving at an optimum value helped in reducing the variance of the weight updates leading to more efficient convergence. I also reduced the number of epochs between 10-30 to arrive at an optimal value. Accuracy increased to about 40%.***\n",
    "   \n",
    "  *** Gradient Descent Optimization: I tried the following SGD methods***\n",
    "  \n",
    " ***1. Velocity+Momentum Gradient descent- best increase in accuracy to about 50%***\n",
    " \n",
    " ***2. Nesterov momentum - Improved the model accuracy by 2-3%***\n",
    " \n",
    " ***3. ADAM method - Improved the model accuracy by 2-3%***\n",
    "   \n",
    "   ***The final accuracy achieved on test set was 50%.***\n",
    "\n",
    "4) **Cross validation** is a technique used to prove the generalization ability of a model and can help you find a robust set of hyperparameters. Please describe the implementation details of **k-fold cross validation** if you want to use it to find a best set of hyperparameter of the **Linear SVM classification** problem.\n",
    "\n",
    "   ***Your answer: 1. Define specific set of hyperparams or range of hyperparams to try out for the SVM.***\n",
    " ***2. Divide the training data into K folds (where k= the no. with least MSE),***\n",
    " \n",
    "    For each of the K folds do the following, \n",
    "    For each set of hyperparams,\n",
    " \n",
    "    a. Train the model on the remaining K-1 folds \n",
    "    b. Calculate the error/asses accuracy on the validation fold (the Kth fold)\n",
    " \n",
    " ***3. Find the model with the average error over all folds for every set of hyperparams associated ***\n",
    "\n",
    " ***4. Pick the set with least average error***\n",
    " \n",
    " ***Hyperparams for linear SVM are C(reg) and epsilon.***\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
